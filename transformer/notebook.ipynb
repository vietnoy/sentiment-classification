{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8784a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6565bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data.data_preprocessing import load_and_preprocess\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "# Transformer Sentiment Classification\n",
    "\n",
    "This notebook implements a Transformer architecture from scratch for sentiment analysis on Amazon product reviews.\n",
    "\n",
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950b7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Amazon dataset\n",
    "train, val, test = load_and_preprocess(train_sample_size=100000, val_sample_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56562864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Einstein: His Life Revealed</td>\n",
       "      <td>walter isaacson has clearly covered the bases ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Totally unreliable measuring directions</td>\n",
       "      <td>the examples of things that can be built are v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>It's Not Just About the Book</td>\n",
       "      <td>i just finished reading the book as the tour d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Playing in the dark</td>\n",
       "      <td>be careful buying this light the invoice and o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Rose-flavored Water, NOT Rosewater</td>\n",
       "      <td>if all you're after is rose flavor for cooking...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  ...                                               text\n",
       "0      1  ...  walter isaacson has clearly covered the bases ...\n",
       "1      0  ...  the examples of things that can be built are v...\n",
       "2      1  ...  i just finished reading the book as the tour d...\n",
       "3      0  ...  be careful buying this light the invoice and o...\n",
       "4      0  ...  if all you're after is rose flavor for cooking...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the data structure\n",
    "print(\"Dataset structure:\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a78febc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 100000\n",
      "Validation size: 20000\n",
      "Test size: 180000\n",
      "\n",
      "Training label distribution:\n",
      "label\n",
      "0    50133\n",
      "1    49867\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training size: {len(train)}\")\n",
    "print(f\"Validation size: {len(val)}\")\n",
    "print(f\"Test size: {len(test)}\")\n",
    "\n",
    "# Check label distribution\n",
    "print(f\"\\nTraining label distribution:\")\n",
    "print(train['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformer_import",
   "metadata": {},
   "source": [
    "## 2. Import Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformer_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom Transformer implementation\n",
    "from src.Simple_transformer import TransformerSentimentClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_preparation",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "data_prep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: walter isaacson has clearly covered the bases in researching one of the most fascinating and brilliant human beings ever born but i often got lost in the \"nitty gritty\" details that at times seemed tr...\n",
      "Sample label: 1\n"
     ]
    }
   ],
   "source": [
    "# Prepare texts and labels for the transformer\n",
    "train_texts = train['text'].tolist()\n",
    "train_labels = train['label'].tolist()\n",
    "\n",
    "val_texts = val['text'].tolist()\n",
    "val_labels = val['label'].tolist()\n",
    "\n",
    "test_texts = test['text'].tolist()\n",
    "test_labels = test['label'].tolist()\n",
    "\n",
    "print(f\"Sample text: {train_texts[0][:200]}...\")\n",
    "print(f\"Sample label: {train_labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_creation",
   "metadata": {},
   "source": [
    "## 4. Create and Configure Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "create_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with:\n",
      "- Vocabulary size: 10000\n",
      "- Max sequence length: 128\n",
      "- Model dimension: 128\n",
      "- Number of attention heads: 8\n",
      "- Number of transformer layers: 4\n",
      "- Feed-forward dimension: 512\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 128\n",
    "D_MODEL = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "DFF = 512\n",
    "DROPOUT_RATE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Create model\n",
    "model = TransformerSentimentClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_length=MAX_LENGTH,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dff=DFF,\n",
    "    rate=DROPOUT_RATE,\n",
    "    num_classes=1  # Binary classification\n",
    ")\n",
    "\n",
    "print(f\"Model created with:\")\n",
    "print(f\"- Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"- Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"- Model dimension: {D_MODEL}\")\n",
    "print(f\"- Number of attention heads: {NUM_HEADS}\")\n",
    "print(f\"- Number of transformer layers: {NUM_LAYERS}\")\n",
    "print(f\"- Feed-forward dimension: {DFF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocab_building",
   "metadata": {},
   "source": [
    "## 5. Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "build_vocab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary...\n",
      "Built vocabulary with 10000 words\n",
      "\n",
      "Vocabulary statistics:\n",
      "Total vocabulary size: 10000\n",
      "Sample words: ['<PAD>', '<UNK>', '<START>', '<END>', 'the', 'and', 'i', 'a', 'to', 'it', 'of', 'this', 'is', 'in', 'for', 'that', 'was', 'you', 'not', 'with']\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from training texts\n",
    "print(\"Building vocabulary...\")\n",
    "model.build_vocabulary(train_texts, vocab_size=VOCAB_SIZE, max_length=MAX_LENGTH)\n",
    "\n",
    "print(f\"\\nVocabulary statistics:\")\n",
    "print(f\"Total vocabulary size: {len(model.word_to_idx)}\")\n",
    "print(f\"Sample words: {list(model.word_to_idx.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text_encoding",
   "metadata": {},
   "source": [
    "## 6. Encode Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "encode_texts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding texts...\n",
      "Data shapes:\n",
      "X_train: (100000, 128), y_train: (100000,)\n",
      "X_val: (20000, 128), y_val: (20000,)\n",
      "X_test: (180000, 128), y_test: (180000,)\n",
      "\n",
      "Original text: walter isaacson has clearly covered the bases in researching one of the most fascinating and brillia...\n",
      "Encoded (first 20 tokens): [6446    1   45  824 1338    4 7727   13 4296   27   10    4  118 1498\n",
      "    5 1324  853 5199  146 2016]\n"
     ]
    }
   ],
   "source": [
    "# Encode texts to sequences of token indices\n",
    "print(\"Encoding texts...\")\n",
    "X_train = model.encode_texts(train_texts, max_length=MAX_LENGTH)\n",
    "X_val = model.encode_texts(val_texts, max_length=MAX_LENGTH)\n",
    "X_test = model.encode_texts(test_texts, max_length=MAX_LENGTH)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = np.array(train_labels, dtype=np.float32)\n",
    "y_val = np.array(val_labels, dtype=np.float32)\n",
    "y_test = np.array(test_labels, dtype=np.float32)\n",
    "\n",
    "print(f\"Data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Show example of encoded text\n",
    "print(f\"\\nOriginal text: {train_texts[0][:100]}...\")\n",
    "print(f\"Encoded (first 20 tokens): {X_train[0][:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_compilation",
   "metadata": {},
   "source": [
    "## 7. Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "compile_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Architecture:\n",
      "Input: (batch_size, 128) - Token sequences\n",
      "Embedding: 10000 -> 128\n",
      "Positional Encoding: 128 positions\n",
      "Transformer Blocks: 4 layers\n",
      "  - Multi-Head Attention: 8 heads\n",
      "  - Feed-Forward: 128 -> 512 -> 128\n",
      "Global Average Pooling: Sequence aggregation\n",
      "Classification Head: 128 -> 256 -> 128 -> 1\n",
      "Output: Binary sentiment prediction\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Build the model with a dummy input to get parameter count\n",
    "model.build(input_shape=(None, MAX_LENGTH))\n",
    "\n",
    "print(\"\\n Model Architecture:\")\n",
    "print(f\"Input: (batch_size, {MAX_LENGTH}) - Token sequences\")\n",
    "print(f\"Embedding: {VOCAB_SIZE} -> {D_MODEL}\")\n",
    "print(f\"Positional Encoding: {MAX_LENGTH} positions\")\n",
    "print(f\"Transformer Blocks: {NUM_LAYERS} layers\")\n",
    "print(f\"  - Multi-Head Attention: {NUM_HEADS} heads\")\n",
    "print(f\"  - Feed-Forward: {D_MODEL} -> {DFF} -> {D_MODEL}\")\n",
    "print(f\"Global Average Pooling: Sequence aggregation\")\n",
    "print(f\"Classification Head: {D_MODEL} -> 256 -> 128 -> 1\")\n",
    "print(f\"Output: Binary sentiment prediction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_creation",
   "metadata": {},
   "source": [
    "## 8. Create TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "create_datasets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 09:30:26.858417: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-06-10 09:30:26.911337: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 3125\n",
      "Validation batches: 625\n",
      "Test batches: 5625\n"
     ]
    }
   ],
   "source": [
    "# Create efficient TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Datasets created successfully!\")\n",
    "print(f\"Training batches: {len(list(train_dataset))}\")\n",
    "print(f\"Validation batches: {len(list(val_dataset))}\")\n",
    "print(f\"Test batches: {len(list(test_dataset))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_setup",
   "metadata": {},
   "source": [
    "## 9. Setup Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "setup_callbacks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training callbacks configured:\n",
      "- Early stopping on validation accuracy (patience=3)\n",
      "- Learning rate reduction on plateau (factor=0.5, patience=2)\n"
     ]
    }
   ],
   "source": [
    "# Training callbacks for better training\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Training callbacks configured:\")\n",
    "print(\"- Early stopping on validation accuracy (patience=3)\")\n",
    "print(\"- Learning rate reduction on plateau (factor=0.5, patience=2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_training",
   "metadata": {},
   "source": [
    "## 10. Train the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Starting Transformer Training...\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n Training completed!\")\n",
    "\n",
    "# Count parameters after training\n",
    "try:\n",
    "    total_params = model.count_params()\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(\"Note: Transformer uses self-attention for parallel processing\")\n",
    "except:\n",
    "    print(\"Model successfully trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_visualization",
   "metadata": {},
   "source": [
    "## 11. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformer_training_history(history):\n",
    "    \"\"\"Plot training curves for Transformer model\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(history.history['loss'], label='Train Loss', color='blue')\n",
    "    ax1.plot(history.history['val_loss'], label='Val Loss', color='red')\n",
    "    ax1.set_title('Transformer Sentiment Classifier - Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(history.history['accuracy'], label='Train Acc', color='blue')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Val Acc', color='red')\n",
    "    ax2.set_title('Transformer Sentiment Classifier - Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_transformer_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_evaluation",
   "metadata": {},
   "source": [
    "## 12. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformer_model(model, test_dataset, y_test):\n",
    "    \"\"\"Evaluate the Transformer model on test set\"\"\"\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Get predictions for classification report\n",
    "    predictions = model.predict(test_dataset, verbose=0)\n",
    "    predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "    true_classes = y_test.astype(int)\n",
    "    \n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_classes, predicted_classes, \n",
    "                              target_names=['Negative', 'Positive']))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    print(cm)\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy = evaluate_transformer_model(model, test_dataset, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_testing",
   "metadata": {},
   "source": [
    "## 13. Test Model with Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_transformer_predictions(model, texts, max_length=128):\n",
    "    \"\"\"Analyze model predictions on sample texts\"\"\"\n",
    "    print(\"\\n TESTING TRANSFORMER WITH SAMPLE TEXTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        # Encode text\n",
    "        encoded = model.encode_texts([text], max_length=max_length)\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = model.predict(encoded, verbose=0)\n",
    "        \n",
    "        # Interpret prediction\n",
    "        sentiment = \"Positive\" if prediction[0][0] > 0.5 else \"Negative\"\n",
    "        confidence = prediction[0][0] if prediction[0][0] > 0.5 else 1 - prediction[0][0]\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Text: '{text[:100]}{'...' if len(text) > 100 else ''}'\")\n",
    "        print(f\"Prediction: {sentiment} (confidence: {confidence:.3f})\")\n",
    "        print(f\"Raw score: {prediction[0][0]:.4f}\")\n",
    "\n",
    "# Test with various sample texts\n",
    "sample_texts = [\n",
    "    \"This product is absolutely fantastic! I love everything about it.\",\n",
    "    \"Terrible quality. Waste of money. Would not recommend.\",\n",
    "    \"Pretty good overall, but could be better for the price.\",\n",
    "    \"Amazing customer service and fast delivery. Very satisfied!\",\n",
    "    \"The product broke after just one day. Very disappointed.\",\n",
    "    \"Okay product, nothing special but does the job.\"\n",
    "]\n",
    "\n",
    "analyze_transformer_predictions(model, sample_texts, MAX_LENGTH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
