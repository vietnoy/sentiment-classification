# Sentiment Classification Project

A comprehensive sentiment analysis project implementing various neural network architectures on Amazon product reviews. This project compares different approaches from simple neural networks to modern transformers for binary sentiment classification.

## ğŸ“Š Dataset

- **Source**: Amazon Polarity Dataset
- **Task**: Binary sentiment classification (Positive/Negative)
- **Size**: ~3.6M training samples, ~400K test samples
- **Preprocessing**: Text cleaning, tokenization, and train/validation split

## ğŸ—ï¸ Project Structure

```
sentiment-classification/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data_preprocessing.py    # Data loading and preprocessing
â”‚   â””â”€â”€ notebook.ipynb          # Data exploration
â”œâ”€â”€ Simple neural network/
â”‚   â”œâ”€â”€ notebook.ipynb          # Simple NN implementation
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ functions.py        # Utility functions
â”‚       â””â”€â”€ neural_network.py   # NN model definition
â”œâ”€â”€ RNN/
â”‚   â”œâ”€â”€ notebook.ipynb          # Vanilla RNN implementation
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ RNN.py             # RNN model
â”‚       â””â”€â”€ Word2Vec.py        # Word embeddings
â”œâ”€â”€ BRNNs/
â”‚   â”œâ”€â”€ notebook.ipynb          # Bidirectional RNN implementation
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ BRNN.py            # BiRNN model
â”‚       â””â”€â”€ Word2Vec.py        # Word embeddings
â”œâ”€â”€ LSTM/
â”‚   â”œâ”€â”€ notebook.ipynb          # LSTM implementation
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ LSTM.py            # LSTM model
â”‚       â””â”€â”€ Word2Vec.py        # Word embeddings
â”œâ”€â”€ transformer/
â”‚   â”œâ”€â”€ notebook.ipynb          # Transformer implementation
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ Simple_transformer.py  # Transformer model
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸš€ Models Implemented

1. **Simple Neural Network**: Basic feedforward network with bag-of-words features
2. **RNN**: Vanilla Recurrent Neural Network for sequence modeling
3. **Bidirectional RNN**: Enhanced RNN processing sequences in both directions
4. **LSTM**: Long Short-Term Memory networks for better long-range dependencies
5. **Transformer**: Attention-based model for state-of-the-art performance

## ğŸ› ï¸ Installation

1. Clone the repository:
```bash
git clone https://github.com/vietnoy/sentiment-classification.git
cd sentiment-classification
```

2. Create virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ“ Usage

### Data Preprocessing
```python
from data.data_preprocessing import load_and_preprocess

train_df, val_df, test_df = load_and_preprocess(
    split_ratio=0.1,
    train_sample_size=10000,
    val_sample_size=2000
)
```

### Running Models
Each model has its own Jupyter notebook in the respective folder:
- Open the desired notebook (e.g., `RNN/notebook.ipynb`)
- Run all cells to train and evaluate the model
- Compare results across different architectures

## ğŸ¯ Features

- **Modular Design**: Each model in separate folder with clean separation
- **Consistent Interface**: Similar preprocessing and evaluation across models
- **Word Embeddings**: Word2Vec implementation for semantic representations
- **Performance Comparison**: Easy comparison between different architectures
- **Extensible**: Easy to add new models and techniques

## ğŸ“ˆ Expected Results

Models are evaluated on accuracy and loss metrics. Generally expected performance order:
1. Transformer (highest accuracy)
2. LSTM
3. Bidirectional RNN
4. Vanilla RNN
5. Simple Neural Network (baseline)

## ğŸ”§ Requirements

- Python 3.7+
- PyTorch
- Transformers
- scikit-learn
- pandas
- numpy
- datasets
- matplotlib
- seaborn

## ğŸ“š Learning Objectives

This project demonstrates:
- Evolution of NLP architectures
- Sequence modeling techniques
- Attention mechanisms
- Text preprocessing pipelines
- Model comparison methodologies

## ğŸ¤ Contributing

Feel free to contribute by:
- Adding new model architectures
- Improving preprocessing techniques
- Enhancing evaluation metrics
- Adding visualization tools

## ğŸ“„ License

This project is open source and available under the MIT License.